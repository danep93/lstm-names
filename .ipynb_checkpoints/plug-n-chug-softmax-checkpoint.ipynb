{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from matplotlib import pyplot\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "from keras.layers.core import Dropout\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn import metrics\n",
    "from keras.preprocessing import sequence\n",
    "from pandas import get_dummies\n",
    "import operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "females_names_path = '/Users/beh502/Downloads/names/female.txt'\n",
    "male_names_path = '/Users/beh502/Downloads/names/male.txt'\n",
    "internet_words_path = '/Users/beh502/Downloads/unigram_freq.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/beh502/Downloads/names/female.txt') as f:\n",
    "    female_lines = f.read().splitlines()\n",
    "with open('/Users/beh502/Downloads/names/male.txt') as f:\n",
    "    male_lines = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_list = list(set(male_lines + female_lines))\n",
    "female_list = list(set(female_lines))\n",
    "male_list = list(set(male_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframes, make equal representation\n",
    "\n",
    "male_df = pd.DataFrame({'word': male_list, 'target': 'male'})\n",
    "female_df = pd.DataFrame({'word': female_list, 'target': 'female'})\n",
    "\n",
    "internet_df = pd.read_csv(internet_words_path) #want 50/50 distribution in data, no bias\n",
    "internet_df = internet_df.drop(['count'], axis=1)\n",
    "internet_df['target'] = 'internet'\n",
    "\n",
    "min_size = min(male_df.shape[0], female_df.shape[0], internet_df.shape[0])\n",
    "male_df = male_df.head(min_size)\n",
    "female_df = female_df.head(min_size)\n",
    "internet_df = internet_df.head(min_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge them, clean up, apply get_dummies\n",
    "name_frames = [male_df, female_df]\n",
    "merged_names_df = pd.concat(name_frames)\n",
    "merged_names_df = merged_names_df.drop_duplicates(subset='word', keep=False)\n",
    "\n",
    "frames = [merged_names_df, internet_df]\n",
    "merged_df = pd.concat(frames)\n",
    "merged_df = merged_df.drop_duplicates(subset='word', keep='first')\n",
    "merged_df = pd.get_dummies(merged_df, columns=['target'])\n",
    "merged_df = merged_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['word'] = merged_df['word'].str.lower()\n",
    "merged_df['word'] = merged_df['word'].str.strip()\n",
    "X = merged_df['word']\n",
    "y = merged_df[['target_male', 'target_female', 'target_internet']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_chars = {x:idx+1 for idx, x in enumerate(set(''.join(X)))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length of word:  15\n"
     ]
    }
   ],
   "source": [
    "max_word_len = np.max([len(x) for x in X])\n",
    "max_features = len(valid_chars) + 1\n",
    "print(\"Max length of word: \", str(max_word_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data_sequences = [[valid_chars[char] for char in word] for word in X]\n",
    "x_data_sequences = sequence.pad_sequences(x_data_sequences, maxlen=max_word_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 18\n",
    "embedding_layer = Embedding(max_features, 1256, input_length=max_word_len)\n",
    "lstm_layer = LSTM(max_features)\n",
    "dropout_layer = Dropout(0.2)\n",
    "dense_layer = Dense(3)\n",
    "softmax_layer = Activation('softmax')\n",
    "\n",
    "model = Sequential([embedding_layer, lstm_layer, dropout_layer, dense_layer, softmax_layer])\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x_data_sequences, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, epochs = 10, validation_split=0.33, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plug n Chug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_index(my_list):\n",
    "    index, value = max(enumerate(my_list), key=operator.itemgetter(1))\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def names_to_softmaxes(listed_names):\n",
    "    name_vectors = [[valid_chars[char] for char in word] for word in listed_names]\n",
    "    name_vectors = sequence.pad_sequences(name_vectors, maxlen=max_word_len)\n",
    "    return model.predict(name_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_valid_chars = {v: k for k, v in valid_chars.items()}\n",
    "\n",
    "def sequence_to_string(sequence):\n",
    "    constructed = ''\n",
    "    inverted_keys = list(inverted_valid_chars.keys())\n",
    "    for num in sequence:\n",
    "        if(num != 0):\n",
    "            constructed += inverted_valid_chars[num]\n",
    "    return constructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['charlie', 'taylor', 'alexander']\n"
     ]
    }
   ],
   "source": [
    "training_names = [sequence_to_string(x) for x in X_train]\n",
    "difficult_names = ['taylor','tylor','charli','charlie','charlie','alex','alexandra','alexander']\n",
    "in_training = set(training_names).intersection(set(difficult_names))\n",
    "print(list(in_training))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word \t \t y_pred \t \t \t \t flat y_pred\n",
      "tylor \t \t [ 0.68323016  0.20387457  0.11289527] \t \t 0\n",
      "charli \t \t [ 0.24883558  0.74952954  0.00163483] \t \t 1\n",
      "charly \t \t [ 0.4942224   0.43937388  0.06640373] \t \t 0\n",
      "alex \t \t [ 0.37422207  0.33759364  0.28818434] \t \t 0\n",
      "alexandra \t [ 0.01046914  0.98052543  0.00900534] \t \t 1\n"
     ]
    }
   ],
   "source": [
    "# feel free to try some names yourself. Just put them in the names array.\n",
    "\n",
    "names = ['tylor','charli','charly','alex','alexandra']\n",
    "softmaxes = names_to_softmaxes(names)\n",
    "header = \"%s \\t \\t %s \\t \\t \\t \\t %s\" % (\"word\", \"y_pred\", \"flat y_pred\")\n",
    "print(header)\n",
    "for i in range(0, len(names)):\n",
    "    domain = names[i]\n",
    "    softmax = softmaxes[i]\n",
    "    rounded_y_pred = get_max_index(softmax)\n",
    "    #this is for pretty printing\n",
    "    if (len(domain) <= 6):\n",
    "        result = \"%s \\t \\t %s \\t \\t %s\" % (domain, softmax, rounded_y_pred)\n",
    "    else:\n",
    "        result = \"%s \\t %s \\t \\t %s\" % (domain, softmax, rounded_y_pred)\n",
    "    print(result)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
